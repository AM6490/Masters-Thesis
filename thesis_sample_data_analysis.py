# -*- coding: utf-8 -*-
"""Thesis Sample Data Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oyzcWZ0seHQdNYwS-u-Z57eC_U2Rfc3A
"""



import os

os.environ["gcs_access_key"] = ""
os.environ["gcs_secret_key"] = ""

import duckdb, os

con = duckdb.connect()

# First install, then load
con.execute("INSTALL httpfs;")
con.execute("LOAD httpfs;")

con.execute("INSTALL iceberg;")
con.execute("LOAD iceberg;")

ak = os.getenv("gcs_access_key")
sk = os.getenv("gcs_secret_key")

con.execute("SET s3_access_key_id = ?;", [ak])
con.execute("SET s3_secret_access_key = ?;", [sk])
con.execute("SET s3_endpoint = 'storage.googleapis.com';")
con.execute("SET s3_url_style = 'path';")
con.execute("SET s3_use_ssl = true;")
con.execute("SET s3_region = 'auto';")

TABLE = "gs://bl-dataproc-resources/warehouse/public_research/contributor_repo_commits_v2"

# sanity check
print(con.execute(f"SELECT COUNT(*) FROM iceberg_scan('{TABLE}');").fetchdf())

!pip install duckdb networkx python-louvain

import os, duckdb, pandas as pd, networkx as nx
from community import community_louvain  # Louvain community detection

# put your keys in the Colab env (or keep using what you already set)
os.environ["gcs_access_key"] = os.environ.get("gcs_access_key") or "YOUR_KEY_ID"
os.environ["gcs_secret_key"] = os.environ.get("gcs_secret_key") or "YOUR_SECRET_KEY"

con = duckdb.connect()
con.execute("INSTALL httpfs; LOAD httpfs;")
con.execute("INSTALL iceberg; LOAD iceberg;")

con.execute("SET s3_access_key_id = ?;", [os.environ["gcs_access_key"]])
con.execute("SET s3_secret_access_key = ?;", [os.environ["gcs_secret_key"]])
con.execute("SET s3_endpoint = 'storage.googleapis.com';")
con.execute("SET s3_url_style = 'path';")
con.execute("SET s3_use_ssl = true;")
con.execute("SET s3_region = 'auto';")

TABLE = "gs://bl-dataproc-resources/warehouse/public_research/contributor_repo_commits_v2"

# Schema
con.execute(f"DESCRIBE SELECT * FROM iceberg_scan('{TABLE}');").df()

# Distinct counts (fast sanity check)
con.execute(f"""
  SELECT
    COUNT(*) AS rows,
    COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors,
    COUNT(DISTINCT repo) AS n_repos
  FROM iceberg_scan('{TABLE}');
""").df()

# Peek some examples (10 random rows)
con.execute(f"""
  SELECT *
  FROM iceberg_scan('{TABLE}')
  USING SAMPLE 10 ROWS;
""").df()

edges_df = con.execute(f"""
  SELECT
    lower(contributor_unique_id_builder_love) AS contributor_id,  -- light normalize
    repo,
    SUM(contributor_contributions)::BIGINT AS weight,
    MIN(data_timestamp) AS first_seen,
    MAX(data_timestamp) AS last_seen
  FROM iceberg_scan('{TABLE}')
  GROUP BY 1,2
""").df()

edges_df.head(), edges_df.shape

# e.g., last 180 days
edges_180 = con.execute(f"""
  SELECT
    lower(contributor_unique_id_builder_love) AS contributor_id,
    repo,
    SUM(contributor_contributions)::BIGINT AS weight
  FROM iceberg_scan('{TABLE}')
  WHERE data_timestamp >= now() - INTERVAL 180 DAY
  GROUP BY 1,2
""").df()

edges_180.shape

use_df = edges_180 if len(edges_180) else edges_df

B = nx.Graph()
B.add_nodes_from(use_df["repo"].unique().tolist(), bipartite=0, kind="repo")
B.add_nodes_from(use_df["contributor_id"].unique().tolist(), bipartite=1, kind="contributor")

for row in use_df.itertuples(index=False):
    B.add_edge(row.contributor_id, row.repo, weight=int(row.weight))

print(f"Repos: {len([n for n,d in B.nodes(data=True) if d['kind']=='repo'])}")
print(f"Contributors: {len([n for n,d in B.nodes(data=True) if d['kind']=='contributor'])}")
print(f"Edges: {B.number_of_edges()}")

TIME_WINDOW_DAYS = 180      # e.g., last 6 months
MIN_COMMITS_PER_PAIR = 2    # only count a repo toward a pair if both did >=2 commits
MAX_CONTRIBS_PER_REPO = 60  # skip repos with >60 contributors in the window
TOP_K_CONTRIBS = 5000       # cap the contributor universe (set None to disable)

print("Params:", TIME_WINDOW_DAYS, MIN_COMMITS_PER_PAIR, MAX_CONTRIBS_PER_REPO, TOP_K_CONTRIBS)

sample_edges = con.execute(f"""
  SELECT
    lower(contributor_unique_id_builder_love) AS contributor_id,
    repo,
    SUM(contributor_contributions)::BIGINT AS weight
  FROM iceberg_scan('{TABLE}')
  WHERE data_timestamp >= now() - INTERVAL 90 DAY
  GROUP BY 1,2
  HAVING repo IN (
    SELECT repo
    FROM iceberg_scan('{TABLE}')
    WHERE data_timestamp >= now() - INTERVAL 90 DAY
    GROUP BY repo
    ORDER BY COUNT(DISTINCT contributor_unique_id_builder_love) DESC
    LIMIT 20
  )
""").df()

sample_edges.head()

import networkx as nx

B = nx.Graph()

# Add repo nodes with bipartite=0, contributor nodes with bipartite=1
B.add_nodes_from(sample_edges["repo"].unique().tolist(), bipartite=0, kind="repo")
B.add_nodes_from(sample_edges["contributor_id"].unique().tolist(), bipartite=1, kind="contributor")

# Add edges with commit weight
for row in sample_edges.itertuples(index=False):
    B.add_edge(row.contributor_id, row.repo, weight=int(row.weight))

print(f"Repos: {len([n for n,d in B.nodes(data=True) if d['kind']=='repo'])}")
print(f"Contributors: {len([n for n,d in B.nodes(data=True) if d['kind']=='contributor'])}")
print(f"Edges: {B.number_of_edges()}")

print(f"Nodes: {B.number_of_nodes()}, Edges: {B.number_of_edges()}")
degrees = dict(B.degree())
sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:20]

import pandas as pd
nx.to_pandas_edgelist(B).to_csv("bipartite_sample.csv", index=False)

con.execute(f"""
  SELECT contributor_unique_id_builder_love AS contributor_id,
         SUM(contributor_contributions) AS commits
  FROM iceberg_scan('{TABLE}')
  GROUP BY 1
  ORDER BY commits DESC
  LIMIT 20;
""").df()

con.execute(f"""
  SELECT repo,
         COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors
  FROM iceberg_scan('{TABLE}')
  GROUP BY 1
  ORDER BY n_contributors DESC
  LIMIT 20;
""").df()

con.execute(f"""
  SELECT repo,
         COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors
  FROM iceberg_scan('{TABLE}')
  GROUP BY 1
  ORDER BY n_contributors DESC
  LIMIT 20;
""").df()

con.execute(f"""
  SELECT COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors,
         COUNT(DISTINCT repo) AS n_repos,
         COUNT(*) AS n_edges
  FROM iceberg_scan('{TABLE}');
""").df()

repo_stats = con.execute(f"""
  SELECT repo,
         COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors,
         SUM(contributor_contributions) AS total_commits
  FROM iceberg_scan('{TABLE}')
  GROUP BY repo
  ORDER BY total_commits DESC
  LIMIT 20;
""").df()

repo_stats

concentration = con.execute(f"""
  WITH contrib_shares AS (
    SELECT
      repo,
      contributor_unique_id_builder_love AS contributor,
      SUM(contributor_contributions) AS contrib_commits,
      SUM(SUM(contributor_contributions)) OVER (PARTITION BY repo) AS repo_total
    FROM iceberg_scan('{TABLE}')
    GROUP BY repo, contributor
  )
  SELECT
    repo,
    COUNT(DISTINCT contributor) AS n_contributors,
    SUM(contrib_commits) AS total_commits,
    SUM(POWER(contrib_commits * 1.0 / repo_total, 2)) AS hhi
  FROM contrib_shares
  GROUP BY repo
  ORDER BY hhi DESC
  LIMIT 20;
""").df()

concentration

multi_repo_conc = con.execute(f"""
  WITH contrib_shares AS (
    SELECT
      repo,
      contributor_unique_id_builder_love AS contributor,
      SUM(contributor_contributions) AS contrib_commits,
      SUM(SUM(contributor_contributions)) OVER (PARTITION BY repo) AS repo_total
    FROM iceberg_scan('{TABLE}')
    GROUP BY repo, contributor
  )
  SELECT
    repo,
    COUNT(DISTINCT contributor) AS n_contributors,
    SUM(contrib_commits) AS total_commits,
    SUM(POWER(contrib_commits * 1.0 / repo_total, 2)) AS hhi
  FROM contrib_shares
  GROUP BY repo
  HAVING COUNT(DISTINCT contributor) > 1  -- only multi-contributor repos
     AND SUM(POWER(contrib_commits * 1.0 / repo_total, 2)) < 1.0  -- drop hhi=1.0
  ORDER BY hhi DESC
  LIMIT 20;
""").df()

multi_repo_conc

top_contribs = con.execute(f"""
  SELECT contributor_unique_id_builder_love AS contributor,
         SUM(contributor_contributions) AS total_commits,
         COUNT(DISTINCT repo) AS repos_touched
  FROM iceberg_scan('{TABLE}')
  GROUP BY contributor
  ORDER BY total_commits DESC
  LIMIT 20;
""").df()

top_contribs

main_repo = con.execute(f"""
  SELECT contributor_unique_id_builder_love AS contributor,
         repo,
         SUM(contributor_contributions) AS commits_in_repo
  FROM iceberg_scan('{TABLE}')
  GROUP BY contributor, repo
  QUALIFY ROW_NUMBER() OVER (
      PARTITION BY contributor
      ORDER BY SUM(contributor_contributions) DESC
  ) = 1
  ORDER BY commits_in_repo DESC
  LIMIT 20;
""").df()

main_repo

# Look at Matter Labs repos
matter = con.execute(f"""
  SELECT repo,
         COUNT(DISTINCT contributor_unique_id_builder_love) AS n_contributors,
         SUM(contributor_contributions) AS total_commits
  FROM iceberg_scan('{TABLE}')
  WHERE repo ILIKE '%matter%'
  GROUP BY repo
  ORDER BY total_commits DESC
""").df()

print(matter)

repo_url = "https://github.com/matter-labs/era-compiler-llvm"  # example, replace with real
top100 = con.execute(f"""
  SELECT contributor_unique_id_builder_love,
         SUM(contributor_contributions) AS commits
  FROM iceberg_scan('{TABLE}')
  WHERE repo ILIKE '{repo_url}%'
  GROUP BY contributor_unique_id_builder_love
  ORDER BY commits DESC
  LIMIT 100
""").df()

print(top100.head(20))

matter_hhi_org = con.execute(f"""
WITH contrib AS (
  SELECT
    lower(contributor_unique_id_builder_love) AS contributor,
    SUM(contributor_contributions)::BIGINT     AS commits
  FROM iceberg_scan('{TABLE}')
  WHERE repo ILIKE 'https://github.com/matter-labs/%'
  GROUP BY 1
),
tot AS (
  SELECT contributor, commits,
         (SELECT SUM(commits) FROM contrib) AS total
  FROM contrib
)
SELECT
  SUM(commits)                                  AS total_commits,
  COUNT(*)                                      AS n_contributors,
  SUM(POWER(commits * 1.0 / total, 2))          AS hhi,
  MAX(commits * 1.0 / total)                    AS top1_share
FROM tot;
""").df()
matter_hhi_org

matter_pairs = con.execute(f"""
  SELECT
    repo,
    lower(contributor_unique_id_builder_love) AS contributor,
    SUM(contributor_contributions)::BIGINT AS commits
  FROM iceberg_scan('{TABLE}')
  WHERE repo ILIKE 'https://github.com/matter-labs/%'
  GROUP BY 1,2
""").df()

print(matter_pairs.head())
print(len(matter_pairs), "edges (repo-contributor pairs)")

import networkx as nx

B = nx.Graph()

# Add repo nodes
repos = matter_pairs["repo"].unique().tolist()
B.add_nodes_from(repos, bipartite="repos")

# Add contributor nodes
contributors = matter_pairs["contributor"].unique().tolist()
B.add_nodes_from(contributors, bipartite="contributors")

# Add edges
for _, row in matter_pairs.iterrows():
    B.add_edge(row["contributor"], row["repo"], weight=row["commits"])

print("Bipartite graph has", B.number_of_nodes(), "nodes and", B.number_of_edges(), "edges")

# Pick top N contributors
top_contributors = (
    matter_pairs.groupby("contributor")["commits"].sum()
    .sort_values(ascending=False)
    .head(200).index
)

# Filter bipartite graph to top contributors + the repos they touched
subB = B.subgraph(top_contributors.tolist() + repos)

# Project contributorâ€“contributor network
G = nx.bipartite.weighted_projected_graph(subB, top_contributors)

print("Projected contributor graph has", G.number_of_nodes(), "nodes and", G.number_of_edges(), "edges")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=0.2, seed=42)

# Draw nodes
nx.draw_networkx_nodes(G, pos, node_size=50, node_color="skyblue")
# Draw edges (width proportional to shared commits)
nx.draw_networkx_edges(G, pos, alpha=0.3)

# Optional: label top 10 by degree
top10 = sorted(G.degree, key=lambda x: x[1], reverse=True)[:10]
labels = {n: n for n, _ in top10}
nx.draw_networkx_labels(G, pos, labels, font_size=8)

plt.axis("off")
plt.show()

H = G.copy()
for u,v,w in list(H.edges(data="weight")):
    if w < 2:   # tune this
        H.remove_edge(u,v)
H.remove_nodes_from([n for n,d in H.degree() if d==0])
print("After pruning:", H.number_of_nodes(), "nodes,", H.number_of_edges(), "edges")

weights = [w for _,_,w in G.edges(data="weight")]
if weights:
    import numpy as np
    q = np.quantile(weights, 0.90)  # keep top 10% edges
    Hq = G.copy()
    for u,v,w in list(Hq.edges(data="weight")):
        if w < q:
            Hq.remove_edge(u,v)
    Hq.remove_nodes_from([n for n,d in Hq.degree() if d==0])
else:
    Hq = G
H = Hq
print("Quantile-pruned:", H.number_of_nodes(), "nodes,", H.number_of_edges(), "edges")

import igraph as ig
import leidenalg

# Build igraph from NetworkX H with FLOAT weights
nodes = list(H.nodes())
idx = {n:i for i,n in enumerate(nodes)}

edges = []
wts = []
for u, v, d in H.edges(data=True):
    edges.append((idx[u], idx[v]))
    wts.append(float(d.get("weight", 1.0)))  # ensure float

g = ig.Graph()
g.add_vertices(len(nodes))
g.vs["name"] = nodes
g.add_edges(edges)
g.es["weight"] = wts

# Leiden partition (RBConfiguration; you can tune resolution_parameter)
part = leidenalg.find_partition(
    g,
    leidenalg.RBConfigurationVertexPartition,
    weights="weight",
    resolution_parameter=1.0,
)
print("Communities:", len(part))

# Map igraph vertex index -> community id
membership = part.membership  # list aligned to g.vs order
comm_by_node = {nodes[i]: membership[i] for i in range(len(nodes))}

import networkx as nx
nx.set_node_attributes(H, comm_by_node, "community")

# Quick peek at sizes
from collections import Counter
sizes = Counter(comm_by_node.values())
print("Top communities:", sizes.most_common(10))

import matplotlib.pyplot as plt

def draw_community(H, community_id, max_nodes=200):
    nodes_c = [n for n, c in nx.get_node_attributes(H, "community").items() if c == community_id]
    Hc = H.subgraph(nodes_c).copy()
    if Hc.number_of_nodes() > max_nodes:
        top_nodes = sorted(Hc.degree, key=lambda x: x[1], reverse=True)[:max_nodes]
        keep = set(n for n,_ in top_nodes)
        Hc = Hc.subgraph(keep).copy()
    pos = nx.spring_layout(Hc, k=0.2, seed=42)
    plt.figure(figsize=(10,10))
    nx.draw_networkx_nodes(Hc, pos, node_size=30, alpha=0.9)
    nx.draw_networkx_edges(Hc, pos, alpha=0.25)
    # label a few hubs
    deg = dict(Hc.degree())
    labels = {n:n for n,_ in sorted(deg.items(), key=lambda x:x[1], reverse=True)[:15]}
    nx.draw_networkx_labels(Hc, pos, labels=labels, font_size=8)
    plt.title(f"Community {community_id} (n={Hc.number_of_nodes()})")
    plt.axis("off"); plt.show()

# Example: draw the largest community
largest_comm = sizes.most_common(1)[0][0]
draw_community(H, largest_comm, max_nodes=200)